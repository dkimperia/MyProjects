В мире технологических возможностей и постоянного быстрорастущего потока данных был представлен такой термин, как большие данные. Большие данные или Big Data представляют собой структурированные и неструктурированные данные огромных размеров, которые обрабатываются различными программными инструментами.

Сам термин впервые был использован редактором журнала Nature Клиффордом Линчем в 2008 году, где он обратил внимание на бурный рост объемов мировой информации. Он дал понимание тому, что более развитые технологии помогут освоению и более качественному анализу больших данных.

Big Data обладает некоторыми признаками, которые зашифровали в правило VVV: volume, velocity, variety. То есть, данные должны обладать объемом, присутствует постоянная обработка обновляющихся данных и возможность данных принимать неоднородные форматы, быть частично структурированными или неструктурированными [1].

Анализируя и сравнивания большой объем информации, часто находятся определенные взаимосвязи. Так, становится возможным прогнозировать дальнейшие события. Вспомогательным элементом для человека в таких ситуациях выступает искусственный интеллект. Предоставление нейросетям возможности анализировать терабайты информации в виде фотографий, видео или прочего, дает определенный результат, например, прогнозирование пробок на дорогах в ближайшее время.

В сравнении с традиционной аналитикой, Big Data имеет значительные преимущества. Появляется возможность обработки всего массива представленных данных, отпадает нужда в сортировке данных, также, данные обрабатываются по мере их поступления [2].

Основными поставщиками данных в России выступают поисковые системы. Для этого у них существует достаточно технологическая база для новых сервисов, а также имеют доступ к большим массивам данных.

Для обработки больших данных были разработаны некоторые методы, которые успешно используются на данный момент. Международная компания McKinsey, которая специализируется на задачах стратегического управления, выделила несколько методов Big Data:

Data Mining (интеллектуальный анализ данных) представляет собой совокупность методов поиска нетривиальной, неизвестной ранее и полезной информации, которая необходима для принятия решений. В состав Data Mining входят классификация, кластерный и регрессионный анализ, анализ отклонений и др.
Краудсорсинг. Происходит передача некоторых задач неопределенному кругу лиц, которые с помощью своего опыта и знаний их решают, при этом трудовой договор не подписывается.
Смешение и интеграция данных. Для проведения глубинного анализа используются данные из различных источников с использованием некоторого набора техник.
Машинное обучение. Используются обучение с учителем и без учителя. На основе моделей статистического анализа создается комплексный прогноз на основе базовых моделей.
Искусственные нейронные сети. В данный метод входят и генетические алгоритмы. С помощью них решаются задачи оптимизации и моделирования за счет случайного подбора, комбинирования искомых параметров. Для этого используются определенные механизмы, которые схожи с естественным отбором в природе.
Распознавание образов.
Прогнозная аналитика.
Имитационное моделирование. Имитацией является численный метод проведения на компьютерах экспериментов с математическими моделями, описывающих поведение сложных систем в течении продолжительных периодов времени. Имитационное моделирование возможно рассматриваться как вид экспериментальных исследований.
Пространственный анализ. Такой вид анализа использует различные геоданные для выявления дополнительной необходимой информации.
Статистический анализ. Производится анализ временных рядов, A/B-тестирование – при таком методе исследования одну группу параметров сравнивают с несколькими тестовыми группами, в которых некоторые показатели подвергаются изменению. Это производится для выявления изменений, которые могут улучшить целевой показатель.
Визуализация аналитических данных. Информация представляется в формате рисунков или диаграмм, в которые могут быть включены элементы анимации или использование интерактивных возможностей. Таким образом можно показать результаты анализа больших данных в удобном виде для восприятия человеком [3].
В докладе The Data Age 2025 от компании IDC было спрогнозировано, что в 2020 году будет сформировано около 40-44 зеттабайтов (1 зеттабайт = 1024 эксабайта; 1 эксабайт = 1 млрд гигабайтов) информации. К 2025 году это число увеличится в 10 раз. На данный момент, происходит активная интеграция Big Data на предприятия. Будущим больших данных представляются технологии Blockchain, переход на облачные хранилища данных, которые сохраняют ресурсы предприятия, внедрение искусственного интеллекта, а также исследование Dark Data – неоцифрованной информации о компании [4].

