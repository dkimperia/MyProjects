Согласно четвертой парадигме, сформулированной Джимом Греем в 2007 году, данные, полученные в результате наблюдений, деятельности людей в промышленности, социальной сфере, экономике и т.д., а также в ходе измерений высокотехнологичными инструментами формируются в процессе интенсивного анализа данных. Увеличение объема и разнообразия данных в различных областях и интенсивное использование данных приводит к разработке методов и инструментов для анализа данных.

Поиск информации - это процесс извлечения структурированных данных из неструктурированных или плохо структурированных документов. Извлечение информации - это разновидность информационного поиска. Изучение структурных данных и комплексных исследований является результатом исследований и применения различных методов и средств. Целевая схема представляет собой схему структуры данных, которая предоставляет информацию, необходимую для решения определенных проблем, извлеченную из различных разнородных информационных ресурсов.

Извлечение информации в основном связано с идентификацией сущностей и отношений. Это один из ключевых этапов предварительной обработки текста, необходимый для реализации более сложных моделей и программ. Сущности должны быть отнесены к некоторым категориям. Особое место в извлечении сущностей занимают проблемы идентификации именованных сущностей и кореференции (разрешение анафорических связей). [1, c 77].

Процесс извлечения информации из разноструктурированных данных и ее приведения к целевой схеме

На рис 1. Изображены основные этапы процесса извлечения сущностей из исходных разноструктурированных коллекций данных, их интеграции для последующего анализа полученной информации для анализа при решении прикладной задачи (класса задач).

Процесс начинается с поиска информационных ресурсов, релевантных задаче, и извлечения из них исходных коллекций данных. Информационные ресурсы могут содержать структурированные данные (базы данных, представленные в различных моделях), слабоструктурированные данные (например, данные из социальных сетей), неструктурированные данные (текст).

На следующем этапе неструктурированные данные (тексты) пропускаются через средства анализа текста, например, Pullenti, Метафраз, AQL, SystemT. В то же время из текстов извлекаются сущности, например, люди, организации, территориальные образования и т.д. Сущности, извлекаемые из текста одним определенным инструментальным средством, всегда соответствуют одной и той же структуре, определенной форматом вывода средства анализа текста. Эта структура называется исходной схемой данных.

Для структурированных данных (извлеченных из баз данных) исходной схемой является схема базы данных. Для слабоструктурированных данных (например, сообщений в социальных сетях) исходная схема формируется путем объединения схемы структурированной части данных и структуры информации, извлеченной из неструктурированной части.

Следующим шагом является интеграция собранных структурированных коллекций и сущностей, извлеченных из текстов, в общую интегрированную коллекцию. Интеграция информации включает в себя несколько второстепенных этапов.

Сравнение элементов схем источника и целевой схемы может производиться с использованием различных методов и средств автоматизации. На основании сравнения между элементов схем создаются правила преобразования данных из исходных схем в целевую. Эти правила затем применяются для трансформации данных. Чтобы установить отношения между сущностями из разных коллекций, используются методы разрешения сущностей, то есть установления сходства сущностей. Слияние данных - это формирование интегрированного представления информации об одной сущности реального мира, получаемой из различных источников данных.

Наконец, последний этап процесса - это анализ и визуализация информации из интегрированной коллекции с использованием существующих средств анализа данных.

Чтобы обеспечить масштабирование процесса с точки зрения объема извлеченных и интегрированных данных, необходимо реализовать его на основе платформы распределенного хранения и обработки больших объемов данных. [2, c 391]

В данной работе в качестве такой платформы рассмотрим Apache Hadoop. Hadoop включает, в частности, распределенную файловую систему HDFS и менеджер ресурсов YARN. В различных дистрибутивах Hadoop встроены некоторые средства анализа текстов. Так, например, в дистрибутив IBM BigInsights встроен язык разработки экстракторов текстовой аналитики AQL. Для обеспечения реализации методов интеграции данных над Hadoop (включая разрешение и слияние сущностей) может использоваться декларативный язык HIL, ориентированный на разрешение и слияние сущностей в Hadoop-инфраструктуре HIL может использоваться для спецификации правил трансформации данных из исходных схем в целевую, с дальнейшим выполнением этих трансформаций в среде Hadoop. HIL компилируется в язык Jaql, который в свою очередь, автоматически переписывается в MapReduce-программы.

В задачах обработки текста на естественном языке выделяют четыре типа функций потерь, характеризующих потери при неправильном принятии решений на основе наблюдаемых данных:

1) функции потерь, реально существующие в мире, например, потеря денег, времени и т.д. (обычно они не известны);

2) функции экспертной оценки (адекватность оценки, релевантность и т.д.);

3) автоматические методы оценки на основе корреляции (например, BLEU – Bilingual Evaluation Understudy, ROUGE – RecallOriented Understudy For Gisting Evaluation, WER – Word Error Rate, mAP – Mean Average Precision). Эти методы предполагают сравнение с более высокими результатами. В начале работы алгоритмов требуется участие экспертов;

4) автоматические "интуитивные" методы (аккуратность, F-мера, AER – Alignment Error Rate). В этом случае также требуется участие экспертов, но результаты ни с чем не сравниваются.

Заключение

Таким образом, извлечение информации является ключевым этапом в построении сложных систем информационного поиска, в том числе вопросно-ответных системах. Несмотря на разнообразие существующих методов извлечения информации из неструктурированных данных, а именно текстовых корпусов, до сих пор не решены ключевые проблемы информационного поиска, связанные с автоматическим построением баз знаний.